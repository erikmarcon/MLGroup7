{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "- Everything\n",
    "- Drop users that have rated very few movies -> How to decide the threshold?\n",
    "- Define the best approach to the collaborative filtering (model-based or memory-based)\n",
    "- Build the recommender system (can it be as simple as kNN?)\n",
    "\n",
    "### DONE\n",
    "- File loading and creation of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from surprise import Reader, Dataset, SVD, KNNWithMeans\n",
    "from surprise.model_selection import cross_validate, GridSearchCV\n",
    "\n",
    "\n",
    "# Not being used yet.\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Movie Titles file\n",
    "def readMovieTitle(file_path):\n",
    "    data_dict = {'Movie_Id' : [], 'Release_Year' : [], 'Title' : []}\n",
    "    data_file = open(file_path, \"r\", encoding='ISO-8859-1')\n",
    "    for line in data_file:            \n",
    "        id, year, title = line.split(',', 2)\n",
    "        data_dict['Movie_Id'].append(id)\n",
    "        data_dict['Release_Year'].append(year)\n",
    "        data_dict['Title'].append(title.rstrip('\\n'))\n",
    "    data_file.close()\n",
    "            \n",
    "    return pd.DataFrame(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset is very large, so there's a flag to load only a handful of rows if necessary\n",
    "# First 100k lines takes 0.2s to load, the whole dataset takes almost 2 minutes\n",
    "def readFile(file_path, rows = 100000, flag = False):\n",
    "    data_dict = {'Cust_Id' : [], 'Movie_Id' : [], 'Rating' : [], 'Date' : []}\n",
    "    data_file = open(file_path, \"r\")\n",
    "    count = 0\n",
    "    for line in data_file:\n",
    "        count += 1\n",
    "        if flag and (count > rows):\n",
    "            break\n",
    "            \n",
    "        if ':' in line:\n",
    "            movidId = line[:-2] # remove the last character ':'\n",
    "            movieId = int(movidId)\n",
    "        else:\n",
    "            customerID, rating, date = line.split(',')\n",
    "            data_dict['Cust_Id'].append(customerID)\n",
    "            data_dict['Movie_Id'].append(movieId)\n",
    "            data_dict['Rating'].append(rating)\n",
    "            data_dict['Date'].append(date.rstrip(\"\\n\"))\n",
    "    data_file.close()\n",
    "            \n",
    "    return pd.DataFrame(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the title of the movies\n",
    "df_title = readMovieTitle('data/movie_titles.csv')\n",
    "df_title['Movie_Id'] = df_title['Movie_Id'].astype(int)\n",
    "df_title.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_title.loc[df_title['Movie_Id'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the user data\n",
    "# User ID, Movie ID, Rating and Date\n",
    "flag_limit = True\n",
    "df1 = readFile('data/combined_data_1.txt', flag = flag_limit)\n",
    "df2 = readFile('data/combined_data_2.txt', flag = flag_limit)\n",
    "df3 = readFile('data/combined_data_3.txt', flag = flag_limit)\n",
    "df4 = readFile('data/combined_data_4.txt', flag = flag_limit)\n",
    "df1['Rating'] = df1['Rating'].astype(float)\n",
    "df2['Rating'] = df2['Rating'].astype(float)\n",
    "df3['Rating'] = df3['Rating'].astype(float)\n",
    "df4['Rating'] = df4['Rating'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the DataFrame with all the data\n",
    "df = df1.copy()\n",
    "df = pd.concat([df2, df3, df4])\n",
    "df.index = np.arange(0,len(df))\n",
    "df.head(10)\n",
    "\n",
    "# Complete Dataframe with the movie titles\n",
    "# Not necessary\n",
    "df = df.merge(df_title, how='left')\n",
    "df = df.loc[:, ['Cust_Id', 'Movie_Id', 'Title', 'Release_Year', 'Rating', 'Date']]\n",
    "df.head(-10)\n",
    "\n",
    "# Too much data, so I'm deleting the dataframes after the merge\n",
    "# In total it uses 14gb of memory\n",
    "del df1, df2, df3, df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the dataframe by ratings\n",
    "ratings_df = df.groupby('Rating')['Rating'].agg(['count'])\n",
    "\n",
    "# Number of unique movies\n",
    "movie_count = df['Movie_Id'].nunique()\n",
    "\n",
    "# Number of unique customers\n",
    "cust_count = df['Cust_Id'].nunique()\n",
    "\n",
    "# Number of ratings\n",
    "rating_count = df['Cust_Id'].count()\n",
    "\n",
    "ax = ratings_df.plot(kind = 'barh', legend = False, figsize = (15,5))\n",
    "\n",
    "plt.title(f'Total pool: {movie_count} Movies, {cust_count:,} customers, {rating_count:,} ratings given', fontsize=18)\n",
    "plt.axis('off')\n",
    "\n",
    "for i in range(1,6):\n",
    "    ax.text(ratings_df.iloc[i-1][0]/4, i-1, 'Rating {}: {:.0f}%'.format(i, ratings_df.iloc[i-1][0]*100 / ratings_df.sum()[0]), color = 'white', weight = 'bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group users by the number of reviews\n",
    "n_df = df.groupby('Cust_Id')['Rating'].agg(['count'])\n",
    "\n",
    "print(n_df['count'].max())\n",
    "\n",
    "n_df.hist(log=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = ['count','mean']\n",
    "\n",
    "df_movie_summary = df.groupby('Movie_Id')['Rating'].agg(f)\n",
    "df_movie_summary.index = df_movie_summary.index.map(int)\n",
    "movie_benchmark = round(df_movie_summary['count'].quantile(0.7),0)\n",
    "drop_movie_list = df_movie_summary[df_movie_summary['count'] < movie_benchmark].index\n",
    "\n",
    "print('Movie minimum times of review: {}'.format(movie_benchmark))\n",
    "\n",
    "df_cust_summary = df.groupby('Cust_Id')['Rating'].agg(f)\n",
    "df_cust_summary.index = df_cust_summary.index.map(int)\n",
    "cust_benchmark = round(df_cust_summary['count'].quantile(0.7),0)\n",
    "drop_cust_list = df_cust_summary[df_cust_summary['count'] < cust_benchmark].index\n",
    "\n",
    "print('Customer minimum times of review: {}'.format(cust_benchmark))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reader = Reader(rating_scale=(1, 5))\n",
    "# svd = SVD()\n",
    "\n",
    "# data = Dataset.load_from_df(df[['Cust_Id', 'Movie_Id', 'Rating']][:], reader)\n",
    "# #data.split(n_folds=3)\n",
    "\n",
    "# cross_validate(svd, data, measures=['RMSE', 'MAE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the favorite movies of a random user (df['Cust_Id'].mode() -> users with most reviews)\n",
    "df.loc[(df['Cust_Id'] == '305344') & (df['Rating'] >= 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataset inside Surprise\n",
    "# Surprise accepts by default only this 3 parameters \\/\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(df[['Cust_Id', 'Movie_Id', 'Rating']], reader)\n",
    "\n",
    "trainingSet = data.build_full_trainset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SVD for recomendation\n",
    "# Copied this online -> don't fully understand.\n",
    "svd = SVD()\n",
    "user_305344 = df_title.copy()\n",
    "user_305344 = user_305344.reset_index()\n",
    "\n",
    "svd.fit(trainingSet)\n",
    "\n",
    "user_305344['Estimate_Score'] = user_305344['Movie_Id'].apply(lambda x: svd.predict(305344, x).est)\n",
    "\n",
    "user_305344 = user_305344.drop('Movie_Id', axis = 1)\n",
    "\n",
    "user_305344 = user_305344.sort_values('Estimate_Score', ascending=False)\n",
    "print(user_305344[['Release_Year','Title', 'Estimate_Score']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using kNN with Means\n",
    "parameters = {\n",
    "    'name' : 'msd',\n",
    "    'user_based' : False, # Compute similarities between items and users\n",
    "    'min_support' : 3\n",
    "}\n",
    "\n",
    "param_grid = {'sim_option' : parameters}\n",
    "\n",
    "algo = KNNWithMeans(sim_options=parameters)\n",
    "\n",
    "algo.fit(trainingSet)\n",
    "\n",
    "# User ID + Movie ID\n",
    "prediction = algo.predict(305344, 9235)\n",
    "print(prediction.est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search to find the best parameters for SVD\n",
    "param_grid = {\n",
    "    \"n_epochs\": [5, 10],\n",
    "    \"lr_all\": [0.002, 0.005],\n",
    "    \"reg_all\": [0.4, 0.6]\n",
    "}\n",
    "gs = GridSearchCV(SVD, param_grid, measures=[\"rmse\", \"mae\"], cv=3)\n",
    "\n",
    "gs.fit(data)\n",
    "\n",
    "print(gs.best_score[\"rmse\"])\n",
    "print(gs.best_params[\"rmse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appplying Best Parameters to predict the score of the same movie and user predicted by kNN\n",
    "algo = gs.best_estimator[\"rmse\"]\n",
    "algo.fit(data.build_full_trainset())\n",
    "\n",
    "prediction = algo.predict(305344, 9235)\n",
    "print(prediction.est)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ds')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (main, Mar 31 2022, 03:37:37) [Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a25df13df3a09e29cbdae6ff7053faf9527e3ea6ab1de3b1c9c24b3626196969"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
